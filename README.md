ğŸ“˜ YEETCODE â€“ Agentic DSA Solver

YEETCODE is an AI-powered Data Structures & Algorithms problem solver built with:

ğŸ¤– Microsoft Autogen (multi-agent orchestration)

ğŸ¦™ Ollama Mistral (local LLM backend)

ğŸ³ Docker (safe sandbox for code execution)

ğŸ–¥ï¸ Streamlit (interactive UI)

YEETCODE can:

Take a DSA problem statement (input by user).

Use an AI Problem Solver agent (Mistral) to:

Plan a solution

Write Python code + test cases

Ask the Code Executor agent to run the code in Docker

Save the solution to a .py file

Stop automatically once the task is done.

ğŸš€ Demo (example run)
user : Write a Python code to reverse a linked list.
DSA_Problem_Solver_Agent :
Plan: I will implement a LinkedList class with a reverse method.
... (code + 3 test cases) ...
STOP


âœ… Code is saved in linked_list_reverse.py
âœ… Tests executed inside Docker
âœ… Task marked complete

ğŸ›  Tech Stack

LLM Orchestration: Autogen

LLM Backend: Ollama Mistral
 (local, no API costs)

Execution Sandbox: Docker (isolated code runs)

Frontend: Streamlit

ğŸ“‚ Project Structure
YEETCODE/
â”‚â”€â”€ agents/
â”‚   â”œâ”€â”€ problem_solver.py      # DSA solver agent
â”‚   â”œâ”€â”€ code_executor_agent.py # Executes code in Docker
â”‚
â”‚â”€â”€ team/
â”‚   â””â”€â”€ dsa_team.py            # Round-robin group chat config
â”‚
â”‚â”€â”€ config/
â”‚   â”œâ”€â”€ settings.py            # Model client (Ollama / Autogen)
â”‚   â”œâ”€â”€ constant.py            # Config: MODEL, STOP token, etc.
â”‚   â”œâ”€â”€ docker_utils.py        # Start/stop Docker helpers
â”‚   â””â”€â”€ docker_executor.py     # Docker code executor
â”‚
â”‚â”€â”€ main.py                    # CLI entrypoint
â”‚â”€â”€ app.py                     # Streamlit UI
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore

âš¡ Installation & Setup
1. Clone the repo
git clone https://github.com/mallsatvik/YEETCODE-Agentic-DSA-Solver.git
cd YEETCODE-Agentic-DSA-Solver

2. Create virtual environment
python -m venv .venv
.\.venv\Scripts\activate      # Windows
# or
source .venv/bin/activate     # macOS/Linux

3. Install dependencies
pip install -r requirements.txt

4. Setup .env

Create a .env file in project root:

# Ollama local server (OpenAI-compatible endpoint)
OLLAMA_BASE_URL=http://localhost:11434/v1

5. Install Ollama + Mistral

Download Ollama

Run:

ollama pull mistral
ollama serve

6. Run the solver

CLI mode:

python main.py


Web UI (Streamlit):

streamlit run app.py

ğŸ¯ Features

ğŸ”„ Multi-agent loop (Planner + Executor)

âœ… Automatic test cases generation

ğŸ’¾ Saves code to .py files

ğŸ›¡ï¸ Sandboxed Docker execution

ğŸ–¼ï¸ Clean Streamlit interface

ğŸ“Œ Roadmap

 Add Judge agent for âœ… / âŒ test reporting

 Add support for C++ / Java execution

 Add problem loaders (LeetCode, Codeforces)

 Add metrics dashboard (success rate, iterations)

ğŸ¤ Contributing

Pull requests are welcome! Please open an issue first to discuss major changes.

ğŸ“œ License

MIT License. See LICENSE for details.

âœ¨ Built with love by @mallsatvik
